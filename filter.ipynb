{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class FaceLandmarksDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str = r'D:\\vhproj\\dt_landmarks\\data_image\\ibug_300W_large_face_landmark_dataset\\labels_ibug_300W_train.xml',\n",
    "        transform=None,\n",
    "        target_size=(224, 224),\n",
    "        bbox_scale_factor=1.2,\n",
    "        apply_augmentation=False\n",
    "    ):\n",
    "        # parse XML\n",
    "        tree = ET.parse(data_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        self.image_filenames = []\n",
    "        self.landmarks = []\n",
    "        self.crops = []\n",
    "        self.transform = transform\n",
    "        self.root_dir = r'D:\\vhproj\\dt_landmarks\\data_image\\ibug_300W_large_face_landmark_dataset'\n",
    "        self.target_size = target_size\n",
    "        self.bbox_scale_factor = bbox_scale_factor\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "\n",
    "        # đọc thông tin file và landmarks\n",
    "        for filename in root[2]:\n",
    "            img_path = os.path.join(self.root_dir, filename.attrib['file'])\n",
    "            self.image_filenames.append(img_path)\n",
    "            self.crops.append(filename[0].attrib)\n",
    "\n",
    "            lm = []\n",
    "            for num in range(68):\n",
    "                x = int(filename[0][num].attrib['x'])\n",
    "                y = int(filename[0][num].attrib['y'])\n",
    "                lm.append([x, y])\n",
    "            self.landmarks.append(lm)\n",
    "\n",
    "        self.landmarks = np.array(self.landmarks, dtype=np.float32)\n",
    "        assert len(self.image_filenames) == len(self.landmarks)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # đọc ảnh, nếu lỗi thì next\n",
    "        while True:\n",
    "            image = cv2.imread(self.image_filenames[index])\n",
    "            if image is None:\n",
    "                index = (index + 1) % len(self.image_filenames)\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        # chuyển về grayscale\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        landmarks = self.landmarks[index]\n",
    "        bbox = self.crops[index]\n",
    "\n",
    "        # tính crop theo bbox và scale factor\n",
    "        x1 = int(bbox['left'])\n",
    "        y1 = int(bbox['top'])\n",
    "        w = int(bbox['width'])\n",
    "        h = int(bbox['height'])\n",
    "        new_w = int(w * self.bbox_scale_factor)\n",
    "        new_h = int(h * self.bbox_scale_factor)\n",
    "        x1 = max(0, x1 - (new_w - w) // 2)\n",
    "        y1 = max(0, y1 - (new_h - h) // 2)\n",
    "        x2 = min(image.shape[1], x1 + new_w)\n",
    "        y2 = min(image.shape[0], y1 + new_h)\n",
    "        cropped = image[y1:y2, x1:x2]\n",
    "\n",
    "        # scale landmarks vào crop\n",
    "        lm_scaled = landmarks - np.array([x1, y1], dtype=np.float32)\n",
    "        # resize\n",
    "        resized = cv2.resize(cropped, self.target_size)\n",
    "        sx = self.target_size[1] / cropped.shape[1]\n",
    "        sy = self.target_size[0] / cropped.shape[0]\n",
    "        lm_resized = np.stack([lm_scaled[:,0] * sx, lm_scaled[:,1] * sy], axis=1).astype(np.float32)\n",
    "\n",
    "        # augmentation (nếu có)\n",
    "        if self.apply_augmentation:\n",
    "            pil_img = Image.fromarray(resized)\n",
    "            orig_w, orig_h = pil_img.size\n",
    "\n",
    "            if np.random.rand() > 0.5:\n",
    "                pil_img = pil_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                lm_resized[:,0] = orig_w - lm_resized[:,0]\n",
    "\n",
    "            angle = np.random.uniform(-30, 30)\n",
    "            pil_img = pil_img.rotate(angle)\n",
    "            rot_mat = cv2.getRotationMatrix2D((orig_w/2, orig_h/2), angle, 1.0)\n",
    "            ones = np.ones((lm_resized.shape[0], 1), dtype=np.float32)\n",
    "            lm_aug = np.hstack([lm_resized, ones])\n",
    "            lm_resized = (rot_mat @ lm_aug.T).T\n",
    "\n",
    "            resized = np.array(pil_img)\n",
    "\n",
    "        # transform ảnh\n",
    "        if self.transform:\n",
    "            resized = self.transform(resized)\n",
    "        else:\n",
    "            resized = torch.tensor(resized, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "        # flatten landmarks thành 1D tensor\n",
    "        landmarks_tensor = torch.tensor(lm_resized.flatten(), dtype=torch.float32)\n",
    "\n",
    "        return resized, landmarks_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class FacialLandmarksModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super(FacialLandmarksModel, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.backbone = resnet18(pretrained=True)\n",
    "        \n",
    "        \n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.backbone.fc = nn.Linear(512, 136)\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        # Lưu lại hàm Loss\n",
    "        self.loss_value = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, landmarks = batch\n",
    "        predictions = self(images)\n",
    "        loss = self.criterion(predictions, landmarks)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, landmarks = batch\n",
    "        predictions = self(images)\n",
    "        val_loss = self.criterion(predictions, landmarks)\n",
    "        self.log('val_loss', val_loss)\n",
    "        self.loss_value.append(val_loss)\n",
    "        return val_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3999, Val: 1333, Test: 1334\n"
     ]
    }
   ],
   "source": [
    "dataset = FaceLandmarksDataset()\n",
    "# Tính kích thước\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.6 * dataset_size)\n",
    "val_size   = int(0.2 * dataset_size)\n",
    "test_size  = dataset_size - train_size - val_size\n",
    "\n",
    "# Chia dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "# Tạo DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# In ra số phần tử cho kiểm tra\n",
    "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | backbone  | ResNet  | 11.2 M | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.960    Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\firek\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Tạo thư mục lưu model nếu chưa có\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Callback: Lưu model tốt nhất\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,\n",
    "    filename=\"best_model\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Callback: Dừng sớm nếu không cải thiện\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Khởi tạo model\n",
    "model = FacialLandmarksModel(learning_rate=1e-3)\n",
    "\n",
    "# Khởi tạo trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.fit(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hàm vẽ loss sau khi train\n",
    "def plot_loss(model):\n",
    "    # Chuyển loss_value sang CPU nếu nó đang ở trên GPU\n",
    "    loss_values = [loss.cpu().item() for loss in model.loss_value]\n",
    "    plt.plot(loss_values, label='Validation Loss')\n",
    "    plt.title('Loss per Validation Step')\n",
    "    plt.xlabel('Validation Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Sau khi huấn luyện\n",
    "plot_loss(model_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hàm vẽ loss sau khi train từ iterate thứ 100\n",
    "def plot_loss(model, start_iter=100):\n",
    "    # Chuyển loss_value sang CPU nếu nó đang ở trên GPU\n",
    "    loss_values = [loss.cpu().item() for loss in model.loss_value]\n",
    "    \n",
    "    # Lấy loss từ iterate thứ 100 trở đi\n",
    "    loss_values = loss_values[start_iter:]\n",
    "    \n",
    "    # Vẽ biểu đồ\n",
    "    plt.plot(loss_values, label='Validation Loss')\n",
    "    plt.title(f'Loss per Validation Step (from step {start_iter})')\n",
    "    plt.xlabel('Validation Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Sau khi huấn luyện\n",
    "plot_loss(model_, start_iter=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model đã huấn luyện từ file checkpoint\n",
    "\n",
    "model.eval()  # Đặt model ở chế độ eval để test\n",
    "dataset = FaceLandmarksDataset('/kaggle/input/ibug-300w/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml')\n",
    "# Chuẩn bị transform cho ảnh test\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Chọn một ảnh từ dataset\n",
    "index =  104  # Bạn có thể thay đổi index để chọn ảnh khác\n",
    "image_path = dataset.image_filenames[index]\n",
    "\n",
    "# Đọc ảnh từ file\n",
    "image = cv2.imread(image_path)\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Lấy bounding box và landmarks từ dataset\n",
    "bbox = dataset.crops[index]\n",
    "landmarks = dataset.landmarks[index]\n",
    "\n",
    "# Crop ảnh theo bounding box\n",
    "x1 = int(bbox['left'])\n",
    "y1 = int(bbox['top'])\n",
    "x2 = int(bbox['left']) + int(bbox['width'])\n",
    "y2 = int(bbox['top']) + int(bbox['height'])\n",
    "\n",
    "cropped_image = image_gray[y1:y2, x1:x2]\n",
    "\n",
    "# Resize ảnh về kích thước đầu vào của mô hình\n",
    "resized_image = cv2.resize(cropped_image, (224, 224))\n",
    "\n",
    "# Chuyển đổi ảnh thành tensor\n",
    "input_image = torch.tensor(resized_image, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0  # Thêm batch size và kênh\n",
    "\n",
    "# Dự đoán landmarks\n",
    "with torch.no_grad():\n",
    "    predicted_landmarks = model(input_image)\n",
    "\n",
    "# Đưa các điểm dự đoán về kích thước ban đầu\n",
    "predicted_landmarks = predicted_landmarks.view(-1, 2).numpy()\n",
    "scale_x = cropped_image.shape[1] / 224.0\n",
    "scale_y = cropped_image.shape[0] / 224.0\n",
    "predicted_landmarks[:, 0] *= scale_x\n",
    "predicted_landmarks[:, 1] *= scale_y\n",
    "\n",
    "# Vẽ các điểm landmarks lên ảnh\n",
    "for (x, y) in predicted_landmarks:\n",
    "    cv2.circle(cropped_image, (int(x), int(y)), 2, (255, 0, 0), -1)\n",
    "\n",
    "# Hiển thị ảnh và các điểm landmarks\n",
    "plt.imshow(cropped_image, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Tạo DataLoader cho tập test\n",
    "test_dataset = FaceLandmarksDataset(data_path='/kaggle/input/ibug-300w/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml', \n",
    "                                    transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load model đã huấn luyện từ file checkpoint\n",
    "model.eval()  # Chuyển model sang chế độ eval để dự đoán\n",
    "\n",
    "# Tính tổng loss và số lượng mẫu\n",
    "total_mse = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "# Vòng lặp qua tập test để dự đoán và tính MSE\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, true_landmarks = batch\n",
    "        predicted_landmarks = model(images)\n",
    "\n",
    "        # Tính MSE cho batch hiện tại\n",
    "        mse = F.mse_loss(predicted_landmarks, true_landmarks, reduction='sum')  # Tổng MSE cho tất cả landmarks\n",
    "        total_mse += mse.item()  # Thêm MSE của batch hiện tại vào tổng MSE\n",
    "        total_samples += images.size(0)  # Cộng số lượng mẫu vào tổng số mẫu\n",
    "\n",
    "# Tính MSE trung bình\n",
    "average_mse = total_mse / total_samples\n",
    "\n",
    "print(f'Mean Squared Error (MSE) trên tập test: {average_mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lưu mô hình dưới dạng .pth\n",
    "torch.save(model.state_dict(), '/kaggle/working/finalpth.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lưu mô hình dưới dạng .ckpt (sử dụng ModelCheckpoint)\n",
    "trainer.save_checkpoint('/kaggle/working/finalckpt.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load model đã huấn luyện từ file checkpoint\n",
    "model = FacialLandmarksModel.load_from_checkpoint('/kaggle/working/facial_landmarks_model_11_10_1.ckpt')\n",
    "model.eval()  # Đặt model ở chế độ eval để test\n",
    "\n",
    "# Chuẩn bị transform cho ảnh test\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Chọn một ảnh từ dataset\n",
    "index = 901# Bạn có thể thay đổi index để chọn ảnh khác\n",
    "image_path = dataset.image_filenames[index]\n",
    "\n",
    "# Đọc ảnh từ file\n",
    "image = cv2.imread(image_path)\n",
    "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Lấy bounding box và landmarks từ dataset\n",
    "bbox = dataset.crops[index]\n",
    "landmarks = dataset.landmarks[index]\n",
    "\n",
    "# Crop ảnh theo bounding box\n",
    "x1 = int(bbox['left'])\n",
    "y1 = int(bbox['top'])\n",
    "x2 = int(bbox['left']) + int(bbox['width'])\n",
    "y2 = int(bbox['top']) + int(bbox['height'])\n",
    "\n",
    "cropped_image = image_gray[y1:y2, x1:x2]\n",
    "\n",
    "# Resize ảnh về kích thước đầu vào của mô hình\n",
    "resized_image = cv2.resize(cropped_image, (224, 224))\n",
    "\n",
    "# Chuyển đổi ảnh thành tensor\n",
    "input_image = torch.tensor(resized_image, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0  # Thêm batch size và kênh\n",
    "\n",
    "# Dự đoán landmarks\n",
    "with torch.no_grad():\n",
    "    predicted_landmarks = model(input_image)\n",
    "\n",
    "# Đưa các điểm dự đoán về kích thước ban đầu\n",
    "predicted_landmarks = predicted_landmarks.view(-1, 2).numpy()\n",
    "scale_x = cropped_image.shape[1] / 224.0\n",
    "scale_y = cropped_image.shape[0] / 224.0\n",
    "predicted_landmarks[:, 0] *= scale_x\n",
    "predicted_landmarks[:, 1] *= scale_y\n",
    "\n",
    "# Vẽ các điểm landmarks lên ảnh\n",
    "for (x, y) in predicted_landmarks:\n",
    "    cv2.circle(cropped_image, (int(x), int(y)), 2, (255, 0, 0), -1)\n",
    "\n",
    "# Hiển thị ảnh và các điểm landmarks\n",
    "plt.imshow(cropped_image, cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tạo DataLoader cho tập test\n",
    "test_dataset = FaceLandmarksDataset(data_path='/kaggle/input/ibug-300w/ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test.xml', \n",
    "                                    transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load model đã huấn luyện từ file checkpoint\n",
    "model.eval()  # Chuyển model sang chế độ eval để dự đoán\n",
    "\n",
    "# Tính tổng loss và số lượng mẫu\n",
    "total_mse = 0.0\n",
    "total_samples = 0\n",
    "\n",
    "# Số lượng điểm mốc\n",
    "number_of_landmarks = true_landmarks.size(1)  # Giả định true_landmarks đã được định nghĩa\n",
    "\n",
    "# Vòng lặp qua tập test để dự đoán và tính MSE\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images, true_landmarks = batch\n",
    "        predicted_landmarks = model(images)\n",
    "\n",
    "        # Tính MSE cho từng ảnh trong batch\n",
    "        mse_per_image = F.mse_loss(predicted_landmarks, true_landmarks, reduction='none')  # Tính MSE cho từng ảnh\n",
    "        mse_per_image = mse_per_image.view(mse_per_image.size(0), -1)  # Đưa về dạng (batch_size, number_of_landmarks * 2)\n",
    "        \n",
    "        # Tính MSE trung bình cho từng ảnh\n",
    "        mse_average_per_image = mse_per_image.sum(dim=1) / number_of_landmarks  # Tính MSE trung bình cho mỗi ảnh\n",
    "        \n",
    "        total_mse += mse_average_per_image.sum().item()  # Cộng dồn tổng MSE trung bình cho từng ảnh\n",
    "        total_samples += images.size(0)  # Cộng số lượng mẫu vào tổng số mẫu\n",
    "\n",
    "# Tính MSE trung bình trên toàn bộ tập test\n",
    "mean_mse_per_image = total_mse / total_samples\n",
    "\n",
    "print(f'MSE trung bình trên mỗi ảnh: {mean_mse_per_image}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load model đã huấn luyện từ file checkpoint\n",
    "model.eval()  # Chuyển model sang chế độ eval để dự đoán\n",
    "\n",
    "# Danh sách để lưu trữ MSE cho từng ảnh\n",
    "mse_per_image_list = []\n",
    "\n",
    "# Số lượng điểm mốc\n",
    "number_of_landmarks = true_landmarks.size(1)  # Giả định true_landmarks đã được định nghĩa\n",
    "\n",
    "# Vòng lặp qua tập test để dự đoán và tính MSE\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(test_dataset)):\n",
    "        # Lấy từng ảnh và nhãn từ test_dataset\n",
    "        image, true_landmarks = test_dataset[idx]  # Lấy ảnh và nhãn\n",
    "        image = image.unsqueeze(0)  # Thêm chiều batch vào ảnh\n",
    "\n",
    "        predicted_landmarks = model(image)  # Dự đoán landmarks\n",
    "\n",
    "        # Tính MSE cho ảnh hiện tại\n",
    "        mse = F.mse_loss(predicted_landmarks, true_landmarks.unsqueeze(0), reduction='mean')  # Tính MSE cho từng ảnh\n",
    "\n",
    "        mse_per_image_list.append(mse.item())  # Thêm MSE vào danh sách\n",
    "\n",
    "# In MSE cho từng ảnh\n",
    "for idx, mse in enumerate(mse_per_image_list):\n",
    "    print(f'Ảnh {idx + 1}: MSE = {mse}')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5827098,
     "sourceId": 9561973,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
